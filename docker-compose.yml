# Common environment variables for Hadoop services
x-hadoop-common: &hadoop-common
  user: root
  environment:
    HADOOP_HOME: /opt/hadoop
    HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
  networks:
    - cluster_network

services:
  # ==================== KAFKA STACK ====================
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.0
    container_name: zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - cluster_network

  kafka:
    image: confluentinc/cp-kafka:7.9.0
    container_name: kafka
    hostname: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
      - "9093:9093"
    networks:
      - cluster_network

  kafka-connect:
    build:
      context: .
      dockerfile: Dockerfile.kafka-connect
    container_name: kafka-connect
    hostname: kafka-connect
    depends_on:
      - kafka
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-cluster
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
    ports:
      - "8083:8083"
    networks:
      - cluster_network

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    hostname: kafka-ui
    depends_on:
      - kafka
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka:9092
    ports:
      - "8080:8080"
    networks:
      - cluster_network

  # ==================== HADOOP HDFS ====================
  
  namenode:
    <<: *hadoop-common
    image: apache/hadoop:3.4.1
    container_name: namenode
    hostname: namenode
    volumes:
      - namenode_data:/opt/hadoop/data/nameNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop:ro
      - ./scripts/start-hdfs.sh:/start-hdfs.sh:ro
    ports:
      - "9870:9870"  # NameNode Web UI
      - "9000:9000"  # HDFS
    command: ["/bin/bash", "/start-hdfs.sh"]

  datanode1:
    <<: *hadoop-common
    image: apache/hadoop:3.4.1
    container_name: datanode1
    hostname: datanode1
    volumes:
      - datanode1_data:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop:ro
      - ./scripts/init-datanode.sh:/init-datanode.sh:ro
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]

  datanode2:
    <<: *hadoop-common
    image: apache/hadoop:3.4.1
    container_name: datanode2
    hostname: datanode2
    volumes:
      - datanode2_data:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop:ro
      - ./scripts/init-datanode.sh:/init-datanode.sh:ro
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]

  # ==================== HADOOP YARN ====================
  
  resourcemanager:
    <<: *hadoop-common
    build: .
    container_name: resourcemanager
    hostname: resourcemanager
    volumes:
      - ./hadoop_config:/opt/hadoop/etc/hadoop:ro
      - ./scripts/start-yarn.sh:/start-yarn.sh:ro
    ports:
      - "8088:8088"  # YARN Web UI
      - "8030:8030"
      - "8031:8031"
      - "8032:8032"
      - "8033:8033"
    depends_on:
      - namenode
    command: ["/bin/bash", "/start-yarn.sh"]

  nodemanager:
    <<: *hadoop-common
    build: .
    container_name: nodemanager
    hostname: nodemanager
    volumes:
      - ./hadoop_config:/opt/hadoop/etc/hadoop:ro
      - ./scripts/start-nodemanager.sh:/start-nodemanager.sh:ro
    ports:
      - "8042:8042"  # NodeManager Web UI
    depends_on:
      - resourcemanager
    command: ["/bin/bash", "/start-nodemanager.sh"]

networks:
  cluster_network:
    driver: bridge

volumes:
  namenode_data:
  datanode1_data:
  datanode2_data:
